lst1<- c(sample(1 : 40, size = 20, replace = F))
lst1
max(lst1)
up01<-1-max(lst1)
lst1<- c(sample(1 : 40, size = 20, replace = F))
lst1
up<-max(lst1)
up
up01<-1-up
lst1<- c(sample(1 : 40, size = 20, replace = F))
lst1
up<-max(lst1)
up
up01<-(1-up)
lst1<- c(sample(1 : 40, size = 20, replace = F))
lst1
up<-max(lst1)
up
1-up
lst1<- c(sample(1 : 40, size = 20, replace = F))
lst1
up<-max(lst1)
up
1-up/(min(lst1-max(lst1)))
1-up/(min(lst1-max(lst1)))
lst1<- c(sample(1 : 40, size = 20, replace = F))
lst1
up<-max(lst1)
up
1-up/(min(lst1-max(lst1)))
lst1<- c(sample(1 : 40, size = 20, replace = F))
lst1
up<-max(lst1)
up
1-up/(min(lst1-max(lst1)))
lst1<- c(sample(1 : 40, size = 20, replace = F))
lst1
up<-max(lst1)
up
1-up/(min(lst1-max(lst1)))
lst1<- c(sample(1 : 40, size = 20, replace = F))
lst1
up<-max(lst1)
up
1-up/(min(lst1-max(lst1)))
my_function <- function(a, b){
d <- a*b
e <- log(d)
f <- sqrt(e)
return(f)
}
my_function(a=2:4,b=4:6)
cls
cl
clr
age <- c(25,35,50)
salary <- c(200000,1200000,2000000)
df <- data.frame("Age"=age,"Salary"=salary,stringsAsFactors = FALSE)
df
age <- c(25,35,50)
salary <- c(200000,1200000,2000000)
df <- data.frame("Age"=age,"Salary"=salary,stringsAsFactors = TRUE)
df
age <- c(25,35,50)
salary <- c(200000,1200000,2000000)
df <- data.frame("Age"=age,"Salary"=salary,stringsAsFactors = FALSE)
df
age <- c(25,35,50)
salary <- c(200000,1200000,2000000)
df <- data.frame("Age"=age,"Salary"=salary,stringsAsFactors = FALSE)
df
age <- c(25,35,50)
salary <- c(200000,1200000,2000000)
df <- data.frame("Age"=age,"Salary"=salary,stringsAsFactors = FALSE)
df
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
dfNorm <- as.data.frame(lapply(df, normalize))
dfNorm <- as.data.frame(lapply(df[1:2], normalize))
dfNorm
age <- c(25,35,50)
salary <- c(200000,1200000,2000000)
df <- data.frame("Age"=age,"Salary"=salary,stringsAsFactors = FALSE)
df
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
dfNorm <- as.data.frame(lapply(df, normalize))
dfNorm <- as.data.frame(lapply(df[1:2], normalize))
dfNorm
age <- c(25,35,50)
salary <- c(200000,1200000,2000000)
df <- data.frame("Age"=age,"Salary"=salary,stringsAsFactors = FALSE)
df
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
dfNorm <- as.data.frame(lapply(df, normalize))
dfNorm <- as.data.frame(lapply(df[1:2], normalize))
#to get data as frame output
#normally it gives list output
dfNorm
age <- c(25,35,50)
salary <- c(200000,1200000,2000000)
df <- data.frame("Age"=age,"Salary"=salary,stringsAsFactors = FALSE)
df
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
dfNorm <- as.data.frame(lapply(df, normalize))
dfNorm <- as.data.frame(lapply(df[2], normalize))
#to get data as frame output
#normally it gives list output
dfNorm
age <- c(25,35,50)
salary <- c(200000,1200000,2000000)
df <- data.frame("Age"=age,"Salary"=salary,stringsAsFactors = FALSE)
df
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
dfNorm <- as.data.frame(lapply(df, normalize))
dfNorm <- as.data.frame(lapply(df["salary"], normalize))
age <- c(25,35,50)
salary <- c(200000,1200000,2000000)
df <- data.frame("Age"=age,"Salary"=salary,stringsAsFactors = FALSE)
df
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
dfNorm <- as.data.frame(lapply(df, normalize))
dfNorm <- as.data.frame(lapply(df["Salary"], normalize))
#to get data as frame output
#normally it gives list output
dfNorm
age <- c(25,35,50)
salary <- c(200000,1200000,2000000)
df <- data.frame("Age"=age,"Salary"=salary,stringsAsFactors = FALSE)
df
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
dfNorm <- as.data.frame(lapply(df, normalize))
dfNorm <- as.data.frame(lapply(df["Salary"], normalize))
#to get data as frame output
#normally it gives list output
dfNorm
age <- c(25,35,50)
salary <- c(200000,1200000,2000000)
df <- data.frame("Age"=age,"Salary"=salary,stringsAsFactors = FALSE)
df
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
dfNorm <- as.data.frame(lapply(df, normalize))
dfNorm <- as.data.frame(lapply(df[1:2], normalize))
#to get data as frame output
#normally it gives list output
dfNorm
new_noralize <- function(x,new_max=1,new_min=0){
a= (((x-min(x))* (new_max-new_min))/(max(x)-min(x)))+new_min
return(a)
}
dfnorm1<- as.data.frame(lapply(df[1:2],normalize))
age <- c(25,35,50)
salary <- c(200000,1200000,2000000)
df <- data.frame("Age"=age,"Salary"=salary,stringsAsFactors = FALSE)
df
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
dfNorm <- as.data.frame(lapply(df, normalize))
dfNorm <- as.data.frame(lapply(df[1:2], normalize))
#to get data as frame output
#normally it gives list output
dfNorm
new_noralize <- function(x,new_max=1,new_min=0){
a= (((x-min(x))* (new_max-new_min))/(max(x)-min(x)))+new_min
return(a)
}
dfnorm1<- as.data.frame(lapply(df[1:2],normalize))
age <- c(25,35,50)
salary <- c(200000,1200000,2000000)
df <- data.frame("Age"=age,"Salary"=salary,stringsAsFactors = FALSE)
df
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
dfNorm <- as.data.frame(lapply(df, normalize))
dfNorm <- as.data.frame(lapply(df[1:2], normalize))
#to get data as frame output
#normally it gives list output
dfNorm
new_noralize <- function(x,new_max=1,new_min=0){
a= (((x-min(x))* (new_max-new_min))/(max(x)-min(x)))+new_min
return(a)
}
dfnorm1<- as.data.frame(lapply(df[1:2],normalize))
dfnorm1
age <- c(25,35,50)
salary <- c(200000,1200000,2000000)
df <- data.frame("Age"=age,"Salary"=salary,stringsAsFactors = FALSE)
df
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
dfNorm <- as.data.frame(lapply(df, normalize))
dfNorm <- as.data.frame(lapply(df[1:2], normalize))
#to get data as frame output
#normally it gives list output
dfNorm
new_noralize <- function(x,new_max=2,new_min=0){
a= (((x-min(x))* (new_max-new_min))/(max(x)-min(x)))+new_min
return(a)
}
dfnorm1<- as.data.frame(lapply(df[1:2],normalize))
dfnorm1
setwd("D:/Year 2/ML/coursework/partA")
library(ggplot2)
library(NbClust)
library(cluster)
library(factoextra)
data <- read.csv("vehicles.csv", header = TRUE, sep = ",")
#View(data)
# Separate the class labels from the attributes
labels <- data[, ncol(data)]
data <- data[, -ncol(data)]
attribute_data <- data[, 2:ncol(data)]
#Normalize data using min max normalization
normalized_data <- apply(attribute_data, 2, function(x) (x - min(x)) / (max(x) - min(x)))
normalized_data <- cbind(normalized_data)
head(normalized_data)
#Add normalized data into a dataframe
df <- data.frame(x = normalized_data[, 3], y = normalized_data[, 4])
#View norm data
#View(normalized_data)
# Calculate z-scores for each column in the dataset
z_scores <- apply(data[, 2:19], 2, scale)
z_scores
# Set a z-score threshold
z_score_threshold <- 3
# Identify rows with z-scores greater than the threshold
outlier_rows <- apply(abs(z_scores) > z_score_threshold, 1, any)
# Remove the outlier rows from the dataset
clean_data <- data[!outlier_rows, ]
#Add new data to a data frame
clean_df <- data.frame(clean_data)
#clean_df <- scale(clean_dat)
write.csv(clean_df,"clean_data.csv",row.names = FALSE)
# Print the number of removed rows
cat("Removed", sum(outlier_rows), "outlier rows\n")
# Print the dimensions of the cleaned dataset
cat("Cleaned dataset dimensions:", dim(clean_data)[1], "rows and", dim(clean_data)[2], "columns")
#View clean_df data frame
#View(clean_df)
#new_data <- read.csv("clean_data.csv", header = TRUE, sep = ",")
#k-means clustering
set.seed(123)
k.max <- 10
results <- data.frame()
for (i in 2:k.max) {
model <- kmeans(clean_df[, 2:19], centers = i, nstart = 25)
results <- rbind(results, c(i, model$tot.withinss))
}
# Clusters using NBclust
set.seed(123)
nb <- NbClust(clean_df[, 2:19], distance = "euclidean", min.nc = 2, max.nc = 10, method = "kmeans")
print(nb)
#fviz_cluster(nb)
# Use of elbow mehtod and it's plot
ggplot(results, aes(x = results[,1], y = results[,2])) +
geom_line() +
labs(x = "Number of clusters", y = "Within cluster sum of squares") +
ggtitle("Elbow Curve")
# Determine the optimal number of clusters using the gap statistic
set.seed(123)
gap_stat <- clusGap(clean_df[, 2:19], FUN = kmeans, K.max = 10, nstart = 25)
print(gap_stat)
# Plot the results of the gap statistic using the plot function
plot(gap_stat)
fviz_gap_stat(gap_stat)
# Run K-means clustering on clean_df with k=2
#set.seed(123)
#kmeans_obj <- kmeans(clean_df[, 2:19], centers=3, nstart=10)
# Create a new data frame with only the rows used in clustering
#clustered_df <- clean_df[kmeans_obj$cluster == 1 | kmeans_obj$cluster == 2 | kmeans_obj$cluster == 3 | kmeans_obj$cluster == 4,]
# Plot the clusters with their corresponding data points
#fviz_cluster(kmeans_obj, data=clustered_df[, 2:19], geom="point", stand=FALSE)
# Determine the optimal number of clusters using the silhouette method
set.seed(123)
silhouette <- silhouette(kmeans(clean_data[, 2:19], centers=2)$cluster, dist(clean_data[, 2:19]))
summary(silhouette)
set.seed(123)
final <- kmeans(clean_df, centers=2)
print(final$centers)
fviz_cluster(final, data=clean_df)
bss <- sum(final$size * dist(final$centers)^2)
tss <- sum(final$withinss) + bss
wss <- final$withinss
ans <- bss/tss
cat("BSS: ",bss)
cat("TSS: ",tss)
cat("BSS/TSS ratio",ans)
cat("WSS: ",sum(wss))
plot(wss)
#Part c
#Use of gap stat method
set.seed(123)
gap_stat <- clusGap(clean_df[,2:19], FUN = kmeans, K.max = 10, nstart = 25)
print(gap_stat)
plot(gap_stat)
fviz_gap_stat(gap_stat)
#Part d
silhouette_plot <- silhouette(final$cluster, dist(clean_df))
plot(silhouette_plot)
silhouette_plot <- silhouette(final$cluster, dist(clean_df))
plot(silhouette_plot)
setwd("D:/Year 2/ML/coursework/partB")
#Defining libraries
library(neuralnet)
library(ggpubr)
library(ggplot2)
data <- read.csv("new_uowdata.csv", header = TRUE)
head(data)
#Extract values into data frame
esp_data <- as.data.frame(data)
head(esp_data)
boxplot(esp_data[,-1])
#Normalize function
normalize <- function(x){
return((x - min(x))/(max(x) - min(x)))
}
#Normalizing extracted data
norm_data <- as.data.frame(lapply(esp_data[,-1], normalize))
head(norm_data)
summary(norm_data)
boxplot(norm_data)
#Split data for training and testing purposes
train_data <- norm_data[1:380,]
test_data <- norm_data[381:470,]
#View(as.data.frame(train_data))
#View(as.data.frame(test_data))
#Creating the training model
#Matrix for input and vector for output
trainm_in <- matrix(,nrow = 0, ncol = 4)
trainm_out <- c()
#Getting records of input value
for (i in 1:length(train_data$X20.00)) {
target <- i+3
#Stopping  the loop
if (target+1 > length(train_data$X20.00)){break}
#Getting new records from splited data for training
input_dat <- train_data$X20.00[i:target]
output_dat <- train_data$X20.00[target+1]
trainm_in <- rbind(trainm_in,input_dat)
trainm_out <- append(trainm_out,output_dat)
}
trainm_in
#Creating the data frame for neural network training
nn_data <- cbind(as.data.frame(trainm_in, trainm_out))
nn_data
neural_network <- neuralnet(trainm_out~V1+V2+V3+V4, data=nn_data,hidden = c(10,2),linear.output = TRUE )
print(neural_network)
plot(neural_network)
test_in <- matrix(, nrow = 0, ncol = 4)
test_out <- c()#Getting data records and availability
for (i in 1:length(test_data$X20.00)) {
target1 <- i+3
if (target1+1 > length(test_data$X20.00)){break}
#Collecting data
input_test <- test_data$X20.00[i:target1]
output_test <- test_data$X20.00[target1+1]
test_in <- rbind(test_in,input_test)
test_out <- append(test_out,output_test)
}
test_df <- as.data.frame(test_in)
tst_df <- as.data.frame(test_out)
#Testing the nerural net
neural_network_result <- compute(neural_network,test_df)
result <- data.frame(actual= test_out,prediction=neural_network_result$net.result)
min_result <- min(esp_data$X20.00)
max_result <- max(esp_data$X20.00)
#Denormalizing the data
denorm <- function(x,min,max){
return((max-min)*x+min)
}
#Comparing NN's data
predict <- denorm(result$prediction,min_result,max_result)
actual <- denorm(result$actual,min_result,max_result)
compare = data.frame(predict,actual)
#print(mean(denorm_data$actual-denorm_data$predict)/denorm_data$actual)
#mean((compare$actual-compare$predict)/compare$actual)
#Deviation for accuracy
dev <- ((compare$actual-compare$predict)/compare$actual)
summary(dev)
mean(dev)
#compare=data.frame(denorm_data$predict,denorm_data$actual,dev)
accuracy = 1 - abs(mean(dev))
accuracy
rmse <- sqrt(mean(predict-actual)^2)
rmse
mae <- mean(abs(compare$predict - compare$actual))
mae
mape <- mean(abs((predict - actual)/actual)) * 100
mape
smape <- function(actual, predict) {
2 * mean(abs(actual - predict) / (abs(actual) + abs(predict)))
}
smape(actual,predict)
ggplot(compare, aes(x = actual, y = predict)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(x = "Actual", y = "Predicted", title = "Predicted vs. Actual Values")
#Defining libraries
library(neuralnet)
library(ggpubr)
library(ggplot2)
#Read data from csv file
data <- read.csv("new_uowdata.csv", header = TRUE)
head(data)
#Extract values into data frame
esp_data <- as.data.frame(data)
head(esp_data)
boxplot(esp_data[,-1])
#Normalize function
normalize <- function(x){
return((x - min(x))/(max(x) - min(x)))
}
#Normalizing extracted data
norm_data <- as.data.frame(lapply(esp_data[,-1], normalize))
head(norm_data)
summary(norm_data)
boxplot(norm_data)
#Split data for training and testing purposes
train_data <- norm_data[1:380,]
test_data <- norm_data[381:470,]
#View(as.data.frame(train_data))
#View(as.data.frame(test_data))
#Creating the training model
#Matrix for input and vector for output
trainm_in <- matrix(,nrow = 0, ncol = 4)
trainm_out <- c()
#Getting records of input value
for (i in 1:length(train_data$X20.00)) {
target <- i+3
#Stopping  the loop
if (target+1 > length(train_data$X20.00)){break}
#Getting new records from splited data for training
input_dat <- train_data$X20.00[i:target]
output_dat <- train_data$X20.00[target+1]
trainm_in <- rbind(trainm_in,input_dat)
trainm_out <- append(trainm_out,output_dat)
}
trainm_in
#Creating the data frame for neural network training
nn_data <- cbind(as.data.frame(trainm_in, trainm_out))
nn_data
neural_network <- neuralnet(trainm_out~V1+V2+V3+V4, data=nn_data,hidden = c(10,2),linear.output = TRUE )
print(neural_network)
plot(neural_network)
#Testing the model
#Creating matrix and vector for data handling
test_in <- matrix(, nrow = 0, ncol = 4)
test_out <- c()#Getting data records and availability
for (i in 1:length(test_data$X20.00)) {
target1 <- i+3
if (target1+1 > length(test_data$X20.00)){break}
#Collecting data
input_test <- test_data$X20.00[i:target1]
output_test <- test_data$X20.00[target1+1]
test_in <- rbind(test_in,input_test)
test_out <- append(test_out,output_test)
}
test_df <- as.data.frame(test_in)
tst_df <- as.data.frame(test_out)
#Testing the nerural net
neural_network_result <- compute(neural_network,test_df)
result <- data.frame(actual= test_out,prediction=neural_network_result$net.result)
min_result <- min(esp_data$X20.00)
max_result <- max(esp_data$X20.00)
#Denormalizing the data
denorm <- function(x,min,max){
return((max-min)*x+min)
}
#Comparing NN's data
predict <- denorm(result$prediction,min_result,max_result)
actual <- denorm(result$actual,min_result,max_result)
compare = data.frame(predict,actual)
#print(mean(denorm_data$actual-denorm_data$predict)/denorm_data$actual)
#mean((compare$actual-compare$predict)/compare$actual)
#Deviation for accuracy
dev <- ((compare$actual-compare$predict)/compare$actual)
summary(dev)
mean(dev)
#compare=data.frame(denorm_data$predict,denorm_data$actual,dev)
accuracy = 1 - abs(mean(dev))
accuracy
rmse <- sqrt(mean(predict-actual)^2)
rmse
mae <- mean(abs(compare$predict - compare$actual))
mae
mape <- mean(abs((predict - actual)/actual)) * 100
mape
smape <- function(actual, predict) {
2 * mean(abs(actual - predict) / (abs(actual) + abs(predict)))
}
smape(actual,predict)
ggplot(compare, aes(x = actual, y = predict)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(x = "Actual", y = "Predicted", title = "Predicted vs. Actual Values")
